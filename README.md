## 🌍 Earthquake Data Pipeline

This project implements an end-to-end earthquake data pipeline using AWS services (Lambda + Glue) to ingest, transform, and analyze earthquake data from the USGS Earthquake API.

---

## Architecture
![Earthquake Data Pipeline](images/Modeldatabases-Page2.jpeg)

## 📌 Overview

The pipeline performs the following steps:

1. Data Ingestion (Lambda)

- Fetches earthquake data (past 1 year) from the USGS API in GeoJSON format.

- Saves the raw data locally or to Amazon S3 as .json.

2. Data Transformation (Glue – PySpark)

- Converts raw JSON data into a clean tabular (Parquet) format with the following fields:

  -time (human-readable datetime)

  -place

  -magnitude

  -longitude, latitude, depth

  -type (event type)

- Saves results as Parquet files.

3. Data Analysis (Glue – PySpark)

- Calculates average magnitude per day.

- Identifies the Top 10 largest earthquakes.

- Counts earthquakes per country/region (parsed from the place string).

- Final analysis results are saved as Parquet files.

## 📂 Output

The following outputs are generated in the /output/ folder:

transformed_table.parquet → Clean earthquake dataset.

avg_magnitude_per_day.parquet → Trends of earthquake intensity over time.

top10_earthquakes.parquet → Largest earthquakes by magnitude.

earthquakes_per_region.parquet → Geographical distribution of earthquakes.

👉 Both the transformed table and all aggregations are saved in Parquet format.

---

## 🚀 How to Run
# Prerequisites

AWS account with permissions for Lambda, Glue, and S3.

Python 3.9+ (for running scripts locally if needed).

Optional: Virtual environment (python -m venv venv) + install dependencies from requirements.txt.

## Step 1 – Data Ingestion (Lambda)

Navigate to the lambda/ folder.

Deploy ingest_lambda.py to AWS Lambda.

Configure the Lambda function to run manually or on a schedule (e.g., CloudWatch Events).

The raw earthquake JSON data will be saved to S3 or locally.

## Step 2 – Data Transformation (Glue)

Upload glue/transform.py to AWS Glue as a job script.

Point the job to the raw JSON data location (S3).

Run the Glue job → produces a transformed dataset in Parquet format.

## Step 3 – Data Analysis (Glue)

Upload glue/analyze.py to AWS Glue as a job script.

Run the job → generates the following outputs in /output/:

Average magnitude per day

Top 10 largest earthquakes

Earthquake counts per region

📊 Sample Outputs

avg_magnitude_per_day.parquet → Trends of earthquake intensity over time.

top10_earthquakes.parquet → Largest earthquakes by magnitude.

earthquakes_per_region.parquet → Geographical distribution of earthquakes.

---

## 📝 Notes

Large datasets are not included; they can be regenerated by running the pipeline.

Sample outputs are provided to illustrate expected results.

Ensure AWS IAM roles have sufficient permissions for S3, Lambda, and Glue.

## 👩‍💻 Author

Revana Rajdhew
